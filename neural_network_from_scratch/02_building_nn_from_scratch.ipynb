{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_dataset_to_numpy\n",
    "import numpy as np\n",
    "import copy\n",
    "from typing import Tuple, Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trian_dataset = r'datasets\\cat_dog_dataset_train.csv'\n",
    "test_dataset = r\"datasets\\cat_dog_dataset_test.csv\"\n",
    "\n",
    "X_train, y_train = load_dataset_to_numpy(trian_dataset)\n",
    "X_test, y_test = load_dataset_to_numpy(trian_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 557\n",
      "Number of testing examples: 557\n",
      "Height/Width of each image: 64\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_set_x shape: (557, 64, 64, 3)\n",
      "train_set_y shape: (557,)\n",
      "test_set_x shape: (557, 64, 64, 3)\n",
      "test_set_y shape: (557,)\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of training examples: \" + str(X_train.shape[0]))\n",
    "print (\"Number of testing examples: \" + str(X_test.shape[0]))\n",
    "print (\"Height/Width of each image: \" + str(X_train[1].shape[0]))\n",
    "print (\"Each image is of size: (\" + str(X_train[1].shape[0]) + \", \" + str(X_train[1].shape[0]) + \", 3)\")\n",
    "print (\"train_set_x shape: \" + str(X_train.shape))\n",
    "print (\"train_set_y shape: \" + str(y_train.shape))\n",
    "print (\"test_set_x shape: \" + str(X_test.shape))\n",
    "print (\"test_set_y shape: \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flatten the arrays into a single vector of shape (height * width * 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test shape after flattening:\n",
      "train: (12288, 557) \n",
      "test: (12288, 557)\n"
     ]
    }
   ],
   "source": [
    "train_set_x_flatten = X_train.reshape(X_train.shape[0], -1).T\n",
    "test_set_x_flatten = X_test.reshape(X_test.shape[0], -1).T\n",
    "print(\"Train and test shape after flattening:\")\n",
    "print(f\"train: {train_set_x_flatten.shape} \\ntest: {test_set_x_flatten.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    z is A scalar or numpy array of any size.\n",
    "    \"\"\"\n",
    "\n",
    "    s = 1 / ( 1 + np.exp(-z) )\n",
    "\n",
    "    return s\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\" Relu activation function\"\"\"\n",
    "    \n",
    "    return max(0.0, z )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training a neural network, we need to initialize the paramters **w** and **b**. There are several options here, we'll use zero initialization for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dimension):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \"\"\"\n",
    "    \n",
    "    w = np.zeros((dimension, 1), dtype=float)\n",
    "    b = float(0)\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "        TODO add docstring explanation\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # activation\n",
    "    A = sigmoid(np.dot(w.T,X) + b) \n",
    "\n",
    "    # calculate cost \n",
    "    cost = -1/m * np.sum( np.dot(np.log(A), Y.T) + np.dot(np.log(1-A), (1-Y.T)))\n",
    "\n",
    "    # Backprop - find grads\n",
    "    dw = 1/m *(np.dot(X,(A - Y).T))\n",
    "    db = 1/m * (np.sum(A -Y))\n",
    "\n",
    "    cost = np.squeeze(np.array(cost))\n",
    "\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w: np.ndarray, b: float, X: np.ndarray, Y: np.ndarray, \n",
    "             num_iterations: int = 100, learning_rate: float = 0.009, \n",
    "             print_cost: bool = False) -> Tuple[Dict, Dict, List[float]]:\n",
    "    \"\"\"\n",
    "    Performs gradient descent to optimize the parameters w and b.\n",
    "\n",
    "    Args:\n",
    "    w (np.ndarray): Initial weight vector\n",
    "    b (float): Initial bias term\n",
    "    X (np.ndarray): Input data, shape (num_features, num_examples)\n",
    "    Y (np.ndarray): True \"label\" vector, shape (1, num_examples)\n",
    "    num_iterations (int): Number of iterations of the optimization loop\n",
    "    learning_rate (float): Learning rate of the gradient descent update rule\n",
    "    print_cost (bool): Print the cost every 100 iterations if set to True\n",
    "\n",
    "    Returns:\n",
    "    params (Dict): Dictionary containing the optimized w and b\n",
    "    grads (Dict): Dictionary containing the gradients of w and b\n",
    "    costs (List[float]): List of all costs computed during the optimization\n",
    "    \"\"\"\n",
    "    w = copy.deepcopy(w)\n",
    "    b = copy.deepcopy(b)\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Forward and backward propagation\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw, db = grads[\"dw\"], grads[\"db\"]\n",
    "        \n",
    "        # Update parameters\n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "                \n",
    "        # Record and print the cost\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            if print_cost:\n",
    "                print(f\"Cost after iteration {i}: {cost:.6f}\")\n",
    "    \n",
    "    params = {\"w\": w, \"b\": b}\n",
    "    grads = {\"dw\": dw, \"db\": db}\n",
    "    \n",
    "    return params, grads, costs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current cost: 1.7686114420497343\n",
      "current cost: 1.7669364854337144\n",
      "current cost: 1.7652621960970987\n",
      "current cost: 1.7635885755009195\n",
      "current cost: 1.7619156251096324\n",
      "current cost: 1.7602433463911067\n",
      "current cost: 1.758571740816643\n",
      "current cost: 1.7569008098609724\n",
      "current cost: 1.7552305550022582\n",
      "current cost: 1.7535609777221062\n",
      "current cost: 1.7518920795055684\n",
      "current cost: 1.7502238618411425\n",
      "current cost: 1.7485563262207844\n",
      "current cost: 1.7468894741399104\n",
      "current cost: 1.7452233070973995\n",
      "current cost: 1.743557826595605\n",
      "current cost: 1.741893034140339\n",
      "current cost: 1.740228931240908\n",
      "current cost: 1.7385655194100955\n",
      "current cost: 1.7369028001641746\n",
      "current cost: 1.7352407750229064\n",
      "current cost: 1.7335794455095528\n",
      "current cost: 1.7319188131508747\n",
      "current cost: 1.7302588794771454\n",
      "current cost: 1.7285996460221336\n",
      "current cost: 1.726941114323142\n",
      "current cost: 1.725283285920977\n",
      "current cost: 1.723626162359978\n",
      "current cost: 1.7219697451880098\n",
      "current cost: 1.7203140359564715\n",
      "current cost: 1.7186590362202905\n",
      "current cost: 1.717004747537943\n",
      "current cost: 1.7153511714714536\n",
      "current cost: 1.7136983095863874\n",
      "current cost: 1.7120461634518684\n",
      "current cost: 1.7103947346405748\n",
      "current cost: 1.7087440247287529\n",
      "current cost: 1.7070940352962107\n",
      "current cost: 1.7054447679263232\n",
      "current cost: 1.703796224206046\n",
      "current cost: 1.7021484057259095\n",
      "current cost: 1.7005013140800243\n",
      "current cost: 1.69885495086609\n",
      "current cost: 1.697209317685394\n",
      "current cost: 1.695564416142819\n",
      "current cost: 1.6939202478468462\n",
      "current cost: 1.6922768144095515\n",
      "current cost: 1.6906341174466215\n",
      "current cost: 1.688992158577353\n",
      "current cost: 1.6873509394246484\n",
      "current cost: 1.6857104616150318\n",
      "current cost: 1.6840707267786457\n",
      "current cost: 1.6824317365492525\n",
      "current cost: 1.680793492564243\n",
      "current cost: 1.6791559964646396\n",
      "current cost: 1.6775192498950962\n",
      "current cost: 1.675883254503904\n",
      "current cost: 1.6742480119429963\n",
      "current cost: 1.672613523867948\n",
      "current cost: 1.6709797919379776\n",
      "current cost: 1.669346817815958\n",
      "current cost: 1.6677146031684156\n",
      "current cost: 1.6660831496655306\n",
      "current cost: 1.6644524589811418\n",
      "current cost: 1.6628225327927553\n",
      "current cost: 1.661193372781537\n",
      "current cost: 1.6595649806323232\n",
      "current cost: 1.6579373580336247\n",
      "current cost: 1.6563105066776207\n",
      "current cost: 1.6546844282601705\n",
      "current cost: 1.653059124480814\n",
      "current cost: 1.6514345970427722\n",
      "current cost: 1.649810847652953\n",
      "current cost: 1.6481878780219477\n",
      "current cost: 1.6465656898640446\n",
      "current cost: 1.6449442848972196\n",
      "current cost: 1.6433236648431468\n",
      "current cost: 1.6417038314271986\n",
      "current cost: 1.6400847863784425\n",
      "current cost: 1.6384665314296567\n",
      "current cost: 1.6368490683173191\n",
      "current cost: 1.635232398781617\n",
      "current cost: 1.6336165245664411\n",
      "current cost: 1.6320014474194078\n",
      "current cost: 1.6303871690918306\n",
      "current cost: 1.6287736913387467\n",
      "current cost: 1.6271610159189127\n",
      "current cost: 1.6255491445948014\n",
      "current cost: 1.6239380791326081\n",
      "current cost: 1.6223278213022514\n",
      "current cost: 1.620718372877372\n",
      "current cost: 1.619109735635342\n",
      "current cost: 1.617501911357261\n",
      "current cost: 1.6158949018279563\n",
      "current cost: 1.6142887088359859\n",
      "current cost: 1.6126833341736455\n",
      "current cost: 1.611078779636959\n",
      "current cost: 1.6094750470256913\n",
      "current cost: 1.6078721381433436\n",
      "current cost: 1.6062700547971511\n"
     ]
    }
   ],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def update(self, w, dw):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def update(self, w, dw):\n",
    "        return w - self.learning_rate * dw\n",
    "\n",
    "class Momentum(Optimizer):\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
    "        super().__init__(learning_rate)\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "\n",
    "    def update(self, w, dw):\n",
    "        if self.v is None:\n",
    "            self.v = np.zeros_like(w)\n",
    "        self.v = self.momentum * self.v - self.learning_rate * dw\n",
    "        return w + self.v\n",
    "\n",
    "class RMSprop(Optimizer):\n",
    "    def __init__(self, learning_rate=0.01, decay_rate=0.99, epsilon=1e-8):\n",
    "        super().__init__(learning_rate)\n",
    "        self.decay_rate = decay_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.s = None\n",
    "\n",
    "    def update(self, w, dw):\n",
    "        if self.s is None:\n",
    "            self.s = np.zeros_like(w)\n",
    "        self.s = self.decay_rate * self.s + (1 - self.decay_rate) * dw**2\n",
    "        return w - (self.learning_rate / (np.sqrt(self.s) + self.epsilon)) * dw\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        super().__init__(learning_rate)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, w, dw):\n",
    "        if self.m is None:\n",
    "            self.m = np.zeros_like(w)\n",
    "            self.v = np.zeros_like(w)\n",
    "        \n",
    "        self.t += 1\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * dw\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (dw**2)\n",
    "        m_hat = self.m / (1 - self.beta1**self.t)\n",
    "        v_hat = self.v / (1 - self.beta2**self.t)\n",
    "        \n",
    "        return w - (self.learning_rate / (np.sqrt(v_hat) + self.epsilon)) * m_hat\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations=100, optimizer=SGD()):\n",
    "    for _ in range(num_iterations):\n",
    "        # Compute gradients (assume we have a function for this)\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        dw, db = grads[\"dw\"], grads['db']\n",
    "        print(f\"current cost: {cost}\")\n",
    "        # Update parameters\n",
    "        w = optimizer.update(w, dw)\n",
    "        b = optimizer.update(b, db.reshape(1, 1)).squeeze()\n",
    "\n",
    "    return w, b\n",
    "\n",
    "# Usage example\n",
    "w = np.random.randn(10, 1)\n",
    "b = np.zeros(1)\n",
    "X = np.random.randn(10, 1000)\n",
    "Y = np.random.randint(0, 2, (1, 1000))\n",
    "\n",
    "# Using different optimizers\n",
    "w_sgd, b_sgd = optimize(w, b, X, Y, optimizer=SGD(learning_rate=0.01))\n",
    "# w_momentum, b_momentum = optimize(w, b, X, Y, optimizer=Momentum(learning_rate=0.01, momentum=0.9))\n",
    "# w_rmsprop, b_rmsprop = optimize(w, b, X, Y, optimizer=RMSprop(learning_rate=0.01))\n",
    "# w_adam, b_adam = optimize(w, b, X, Y, optimizer=Adam(learning_rate=0.01))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".nnvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
