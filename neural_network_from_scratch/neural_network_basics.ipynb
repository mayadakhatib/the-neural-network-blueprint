{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook you'll learn about what a **neuron** is, how a **logisitc regression** can be represented as a neural network, what is an **activations function** and what are the differnt types we can use "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A single Neuron\n",
    "\n",
    "Before looking at artificial neurons, let's try to understand how an actual biological neuron works (this is a very simplified explanation) compared to an artificial one\n",
    "\n",
    "**Biological Neuron:** A biological neuron is a cell in the nervous system that processes and transmits information. It consists of:\n",
    "\n",
    "- Dendrites: Branch-like structures that receive signals from other neurons.\n",
    "- Cell Body (Soma): The central part of the neuron that processes incoming signals.\n",
    "- Axon: A long fiber that transmits the processed signal to other neurons.\n",
    "\n",
    "**Artificial Neuron:** An artificial neuron is a mathematical model inspired by biological neurons. It has:\n",
    "\n",
    "- Inputs (x₁, x₂, x₃): Represent features or signals from other neurons.\n",
    "- Summation (Σ): The weighted sum of inputs.\n",
    "- Activation Function: Applies a non-linear transformation to the sum (not explicitly shown in this simple model).\n",
    "- Output (y): The result of the neuron's computation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![try](assets/neuron_actual_vs_artificial.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights and biases:\n",
    "**Weights** are assigned to each connection between an input and an output neuron (w₁, w₂, w₃). They determine the strenght of the connection, or in other words, to what extend the combination of these input features affect the output.  \n",
    "\n",
    "While weights determine the strength of connections between neurons, **biases** provide an additional layer of flexibility to neural networks. Biases are constants that serve as a form of offset allowing neurons to activate even when the weighted sum of their inputs is not sufficient on its own. Unlike weights, they are not related to inputs but are added to the neurons' outputs \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![wieghts_biases](assets/weights_biases.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy representation of a neuron \n",
    "Let's create a simple neuron with forward pass only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3.]\n",
      "0.017728170407564765\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, num_inputs):\n",
    "        # Initialize weights randomly from a normal distribution\n",
    "        self.weights = np.random.randn(num_inputs)\n",
    "        # Initialize bias to zero\n",
    "        self.bias = 0\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        # Sigmoid activation function\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Ensure inputs is a numpy array\n",
    "        inputs = np.array(inputs)\n",
    "        \n",
    "        # Calculate the weighted sum\n",
    "        z = np.dot(self.weights, inputs) + self.bias\n",
    "        \n",
    "        # Apply the activation function\n",
    "        return self.sigmoid(z)\n",
    "    \n",
    "neuron = Neuron(3)\n",
    "\n",
    "inputs = np.array([1.0, 2.0, 3.0])\n",
    "outputs = neuron.forward(inputs)\n",
    "print(inputs)\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network\n",
    "\n",
    "A **neural network** consists of layers of neurons: an input layer, one or more hidden layers, and an output layer. By combining weighted inputs, the network learns how different features interact and influence the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](assets/Neural-Networks-Architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A common example: Logistic regression as neural network\n",
    "A logistic regression can be represented as a single-layer neural network with a **sigmoid activation function**.   \n",
    "\n",
    "![image.png](assets/attachment:image.png)\n",
    "\n",
    "The **sigmoid function** maps any input to a value between 0 and 1.\n",
    "- Formula: σ(z) = 1 / (1 + e⁻ˣ)\n",
    "- Shape: An S-shaped curve that approaches 0 for large negative values and 1 for large positive values.  \n",
    "| large z &rarr;  σ(z) $\\approx$ 1  \n",
    "| small z &rarr;  σ(z) $\\approx$ 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Cost Function\n",
    "\n",
    "Unlike linear regression, for logistic regression we don't use a squared error cost function since the optimization problem we're solving is **not** convex. This means it can have multiple local minima, making it difficult for gradient descent (optimizer) to find the global minimum.  \n",
    "\n",
    "![image](assets/convex_vs_non_convex.png)\n",
    "\n",
    "\n",
    "Cost function for a single training example:\n",
    "\n",
    "$$J(\\theta) = -[y \\log(h_\\theta(x)) + (1-y) \\log(1-h_\\theta(x))]$$\n",
    "\n",
    "For a dataset of m training examples:\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m [y^{(i)} \\log(h_\\theta(x^{(i)})) + (1-y^{(i)}) \\log(1-h_\\theta(x^{(i)}))]$$\n",
    "\n",
    "Where:\n",
    "- $J(\\theta)$ is the cost function\n",
    "- $\\theta$ represents the parameters (weights and bias)\n",
    "- $h_\\theta(x)$ is the logistic function (sigmoid) applied to the linear combination of inputs\n",
    "- $y$ is the true label (0 or 1)\n",
    "- $x$ is the input feature vector\n",
    "- $m$ is the number of training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent - the OG Optimization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression from scartch using python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial cost: 0.893624983899058\n",
      "Final cost: 0.28594944433966396\n",
      "Accuracy: 0.98\n",
      "Predictions for new data: [1 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class LogisticRegressionNN:\n",
    "    def __init__(self, input_size):\n",
    "        self.weights = np.random.randn(input_size, 1)\n",
    "        self.bias = np.zeros((1, 1))\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return self.sigmoid(z)\n",
    "    \n",
    "    def compute_cost(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        h = self.forward(X)\n",
    "        epsilon = 1e-15  # Small value to avoid log(0)\n",
    "        cost = (-1/m) * np.sum(y * np.log(h + epsilon) + (1-y) * np.log(1-h + epsilon))\n",
    "        return cost\n",
    "    \n",
    "    def compute_gradients(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        h = self.forward(X)\n",
    "        dw = (1/m) * np.dot(X.T, (h - y))\n",
    "        db = (1/m) * np.sum(h - y)\n",
    "        return dw, db\n",
    "    \n",
    "    def train(self, X, y, learning_rate=0.01, epochs=1000):\n",
    "        for _ in range(epochs):\n",
    "            dw, db = self.compute_gradients(X, y)\n",
    "            self.weights -= learning_rate * dw\n",
    "            self.bias -= learning_rate * db\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return (self.forward(X) >= threshold).astype(int)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate some sample data\n",
    "    np.random.seed(0)\n",
    "    X = np.random.randn(100, 2)\n",
    "    y = ((X[:, 0] + X[:, 1]) > 0).astype(int).reshape(-1, 1)\n",
    "\n",
    "    # Create and train the model\n",
    "    model = LogisticRegressionNN(input_size=2)\n",
    "    initial_cost = model.compute_cost(X, y)\n",
    "    print(f\"Initial cost: {initial_cost}\")\n",
    "\n",
    "    model.train(X, y)\n",
    "    final_cost = model.compute_cost(X, y)\n",
    "    print(f\"Final cost: {final_cost}\")\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(X)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    # Test with new data\n",
    "    X_new = np.array([[1, 2], [-1, -2]])\n",
    "    predictions_new = model.predict(X_new)\n",
    "    print(f\"Predictions for new data: {predictions_new.flatten()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".nnvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
